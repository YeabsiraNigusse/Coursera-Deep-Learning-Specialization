{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHQrJbZYfFnkUDp18yxIxk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YeabsiraNigusse/Coursera-Deep-Learning-Specialization/blob/main/Course%202/Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Course 2 Notes"
      ],
      "metadata": {
        "id": "0jJkteGCKdQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main points fro the lectures\n",
        "- Deep learning is iterative process\n",
        "- we try different values for hyperparameter like Layers, learning rate, activation function and number of hidden layer.\n",
        "-separating our data into train, dev and test set with traditional ratio 60%/ 20%/ 20% or if thge dataset is big it can be 99%/ 0.5%/ 0.5% make our iterative stap faster to get a good model\n",
        "- dev set help us to evaluate different algorithms or models persormance\n",
        "- the key elements to evaluate our model as high or low variance and bias are train and dev set error.\n",
        "- `Hign Bias` means the model underfites train data set.\n",
        "- `Hign Variance` mean it underfits on our dev data set\n",
        "- we masure the train and dev set based on optimal error\n",
        "- if you have hign bias try large nural network\n",
        " if you have hign variance try more data and regularization"
      ],
      "metadata": {
        "id": "5JsfDYkgzvF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Regularization"
      ],
      "metadata": {
        "id": "ZADq69SrP7X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization reduce overfitting by making the cost function simpler by using two techniqes.\n",
        "\n",
        "`L1 (Lasso) Regularization:`\n",
        "\n",
        "- L1 regularization adds a penalty term based on the absolute values of the model's coefficients. It encourages sparsity, meaning it drives some coefficients to become exactly zero, effectively removing certain features from the model.\n",
        "\n",
        "The cost function for a linear regression model with L1 regularization can be expressed as:\n",
        "\n",
        "Cost = Mean Squared Error (MSE) + λ * Σ|βi|\n",
        "\n",
        "`L2 (Ridge) Regularization:`\n",
        "- L2 regularization adds a penalty term based on the square of the model's coefficients. It discourages large coefficient values.\n",
        "\n",
        "The cost function for a linear regression model with L2 regularization can be expressed as:\n",
        "\n",
        "Cost = Mean Squared Error (MSE) + λ * Σ(βi^2)"
      ],
      "metadata": {
        "id": "yCxUAz6RQEUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout Regularization\n",
        "\n",
        "\n",
        "it is way of reducing regularization with intentional remval of node from the network which results reducing the complexity of the network\n",
        "\n",
        "`Inverted Droupout`"
      ],
      "metadata": {
        "id": "gz08s93c5t7w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "drIDvph_KiDy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}